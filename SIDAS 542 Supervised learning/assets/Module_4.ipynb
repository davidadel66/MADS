{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently using **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SIADS 542: Supervised Learning, Week 4:  Tree-based classifiers, Naive Bayes, Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either of the following is no longer\n",
    "# necessary for matplotlib in notebooks.\n",
    "# The import statement has you covered!\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings only when absolutely necessary\n",
    "# Warnings are in place for a reason!\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from adspy_shared_utilities import load_crime_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "## Additional imports can be inlcuded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cmap_bold = ListedColormap([\"#FFFF00\", \"#00FF00\", \"#0000FF\", \"#000000\"])\n",
    "cmap_bold = ListedColormap([\"#FF8C00\", \"#00FF00\", \"#0000FF\", \"#000000\"])\n",
    "\n",
    "# fruits dataset\n",
    "fruits = pd.read_table(\"fruit_data_with_colors.txt\")\n",
    "\n",
    "feature_names_fruits = [\"height\", \"width\", \"mass\", \"color_score\"]\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits[\"fruit_label\"]\n",
    "target_names_fruits = [\"apple\", \"mandarin\", \"orange\", \"lemon\"]\n",
    "\n",
    "X_fruits_2d = fruits[[\"height\", \"width\"]]\n",
    "y_fruits_2d = fruits[\"fruit_label\"]\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Sample regression problem with one input variable\")\n",
    "plt.grid(alpha=0.2)\n",
    "X_R1, y_R1 = make_regression(\n",
    "    n_samples=100, n_features=1, n_informative=1, bias=150.0, noise=30, random_state=0\n",
    ")\n",
    "plt.scatter(X_R1, y_R1, marker=\"o\", s=50)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Complex regression problem with one input variable\")\n",
    "plt.grid(alpha=0.2)\n",
    "X_F1, y_F1 = make_friedman1(n_samples=100, n_features=7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker=\"o\", s=50)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# synthetic dataset for classification (binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Sample binary classification problem with two informative features\")\n",
    "plt.grid(alpha=0.2)\n",
    "X_C2, y_C2 = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,\n",
    "    class_sep=0.5,\n",
    "    random_state=0,\n",
    ")\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], marker=\"o\", c=y_C2, s=50, cmap=cmap_bold)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# more difficult synthetic dataset for classification (binary)\n",
    "# with classes that are not linearly separable\n",
    "X_D2, y_D2 = make_blobs(\n",
    "    n_samples=100, n_features=2, centers=8, cluster_std=1.3, random_state=4\n",
    ")\n",
    "y_D2 = y_D2 % 2\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Sample binary classification problem with non-linearly separable classes\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.scatter(X_D2[:, 0], X_D2[:, 1], c=y_D2, marker=\"o\", s=50, cmap=cmap_bold)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Breast cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Communities and Crime dataset\n",
    "(X_crime, y_crime) = load_crime_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plot_class_regions_for_classifier(\n",
    "    nbclf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"Gaussian Naive Bayes classifier: Dataset 1\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_class_regions_for_classifier(\n",
    "    nbclf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"Gaussian Naive Bayes classifier: Dataset 2\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "print(\"Breast cancer dataset\")\n",
    "print(\n",
    "    \"Accuracy of GaussianNB classifier on training set: {:.3f}\".format(\n",
    "        nbclf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of GaussianNB classifier on test set: {:.3f}\".format(\n",
    "        nbclf.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=3\n",
    ")\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Accuracy of Decision Tree classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of Decision Tree classifier on test set: {:.3f}\".format(\n",
    "        clf.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting max decision tree depth to help avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Accuracy of Decision Tree classifier on training set: {:.3f}\".format(\n",
    "        clf2.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of Decision Tree classifier on test set: {:.3f}\".format(\n",
    "        clf2.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-pruned version (max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf2, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plot_feature_importances(clf, iris.feature_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"Feature importances: {}\".format(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "pair_list = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "tree_max_depth = 4\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=0\n",
    ")\n",
    "\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(8, 32))\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "\n",
    "    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n",
    "    title = \"Decision Tree, max_depth = {:d}\".format(tree_max_depth)\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        clf, X, y, None, None, title, axis, iris.target_names\n",
    "    )\n",
    "\n",
    "    axis.set_xlabel(iris.feature_names[pair[0]])\n",
    "    axis.set_ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4, min_samples_leaf=8, random_state=0).fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "plot_decision_tree(clf, cancer.feature_names, cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Breast cancer dataset: decision tree\")\n",
    "print(\n",
    "    \"Accuracy of DT classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\"Accuracy of DT classifier on test set: {:.3f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plot_feature_importances(clf, cancer.feature_names)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "title = \"Random Forest Classifier, complex binary dataset, default settings\"\n",
    "plot_class_regions_for_classifier_subplot(\n",
    "    clf, X_train, y_train, X_test, y_test, title, subaxes\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest: Fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fruits.values, y_fruits.values, random_state=0\n",
    ")\n",
    "\n",
    "pair_list = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(8, 32))\n",
    "title = \"Random Forest, fruits dataset, default settings\"\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "\n",
    "    clf = RandomForestClassifier().fit(X, y)\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        clf, X, y, None, None, title, axis, target_names_fruits\n",
    "    )\n",
    "\n",
    "    axis.set_xlabel(feature_names_fruits[pair[0]])\n",
    "    axis.set_ylabel(feature_names_fruits[pair[1]])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest, Fruit dataset, default settings\")\n",
    "print(\n",
    "    \"Accuracy of RF classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\"Accuracy of RF classifier on test set: {:.3f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(max_features=8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Breast cancer dataset\")\n",
    "print(\n",
    "    \"Accuracy of RF classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\"Accuracy of RF classifier on test set: {:.3f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "title = \"GBDT, complex binary dataset, default settings\"\n",
    "plot_class_regions_for_classifier_subplot(\n",
    "    clf, X_train, y_train, X_test, y_test, title, subaxes\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosted decision trees on the fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fruits.values, y_fruits.values, random_state=0\n",
    ")\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(8, 32))\n",
    "\n",
    "pair_list = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "\n",
    "    clf = GradientBoostingClassifier().fit(X, y)\n",
    "\n",
    "    title = f\"GBDT, complex binary dataset, pairing: {pair}\"\n",
    "\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        clf, X, y, None, None, title, axis, target_names_fruits\n",
    "    )\n",
    "\n",
    "    axis.set_xlabel(feature_names_fruits[pair[0]])\n",
    "    axis.set_ylabel(feature_names_fruits[pair[1]])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"GBDT, Fruit dataset, default settings\")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on test set: {:.3f}\".format(clf.score(X_test, y_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-boosted decision trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Breast cancer dataset (learning_rate=0.1, max_depth=3)\")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on test set: {:.3f}\\n\".format(\n",
    "        clf.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Breast cancer dataset (learning_rate=0.01, max_depth=2)\")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of GBDT classifier on test set: {:.3f}\".format(clf.score(X_test, y_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-2, 2, 200)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.title(\"Neural network activation functions\")\n",
    "plt.xlabel(\"Input value (x)\")\n",
    "plt.ylabel(\"Activation function output\")\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.plot(xrange, np.maximum(xrange, 0), label=\"relu\")\n",
    "plt.plot(xrange, np.tanh(xrange), label=\"tanh\")\n",
    "plt.plot(xrange, 1 / (1 + np.exp(-xrange)), label=\"logistic\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic dataset 1: single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# default: max_iter=200\n",
    "# try increasing max_iter to eliminate the warning\n",
    "# How far do you have to go to attain convergence with all models?\n",
    "max_iter = 200\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 24))\n",
    "\n",
    "for units, axis in zip([1, 10, 100], axes):\n",
    "    nnclf = MLPClassifier(\n",
    "        hidden_layer_sizes=[units],\n",
    "        solver=\"lbfgs\",\n",
    "        random_state=0,\n",
    "        max_iter=max_iter,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    title = \"Dataset 1: Neural net classifier, 1 layer, {} units\".format(units)\n",
    "\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        nnclf, X_train, y_train, X_test, y_test, title, axis\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic dataset 1: two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "nnclf = MLPClassifier(\n",
    "    hidden_layer_sizes=[10, 10],\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=0,\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plot_class_regions_for_classifier(\n",
    "    nnclf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"Dataset 1: Neural net classifier, 2 layers, 10/10 units\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# default: max_iter=200\n",
    "# try increasing max_iter to eliminate the warning\n",
    "# How far do you have to go to attain convergence with all models?\n",
    "# How are the class region plots affected?\n",
    "max_iter = 200\n",
    "\n",
    "fig, subaxes = plt.subplots(4, 1, figsize=(8, 24))\n",
    "\n",
    "for this_alpha, axis in zip([0.01, 0.1, 1.0, 5.0], subaxes):\n",
    "    nnclf = MLPClassifier(\n",
    "        solver=\"lbfgs\",\n",
    "        activation=\"tanh\",\n",
    "        alpha=this_alpha,\n",
    "        hidden_layer_sizes=[100, 100],\n",
    "        max_iter=max_iter,\n",
    "        random_state=0,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    title = \"Dataset 2: NN classifier, alpha = {:.3f} \".format(this_alpha)\n",
    "\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        nnclf, X_train, y_train, X_test, y_test, title, axis\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The effect of different choices of activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# default: max_iter=200\n",
    "# try increasing max_iter to eliminate the warning\n",
    "# How far do you have to go to attain convergence with all models?\n",
    "# How are the class region plots affected?\n",
    "max_iter = 200\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(8, 18))\n",
    "\n",
    "for this_activation, axis in zip([\"logistic\", \"tanh\", \"relu\"], subaxes):\n",
    "    nnclf = MLPClassifier(\n",
    "        hidden_layer_sizes=[10, 10],\n",
    "        solver=\"lbfgs\",\n",
    "        activation=this_activation,\n",
    "        alpha=0.1,\n",
    "        max_iter=max_iter,\n",
    "        random_state=0,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    title = f\"Dataset 2: NN classifier, 2 layers 10/10, {this_activation} \\\n",
    "activation function, n_iters: {nnclf.n_iter_}\"\n",
    "\n",
    "    plot_class_regions_for_classifier_subplot(\n",
    "        nnclf, X_train, y_train, X_test, y_test, title, axis\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# default: max_iter=200\n",
    "# try increasing max_iter to eliminate the warning\n",
    "# How far do you have to go to attain convergence with all models?\n",
    "max_iter = 200\n",
    "\n",
    "X_predict_input = np.linspace(-3, 3, 50).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_R1[0::5], y_R1[0::5], random_state=0\n",
    ")\n",
    "\n",
    "fig, subaxes = plt.subplots(2, 3, figsize=(11, 8))\n",
    "\n",
    "for thisaxisrow, thisactivation in zip(subaxes, [\"tanh\", \"relu\"]):\n",
    "    for thisalpha, thisaxis in zip([0.0001, 1.0, 100], thisaxisrow):\n",
    "        mlpreg = MLPRegressor(\n",
    "            hidden_layer_sizes=[100, 100],\n",
    "            activation=thisactivation,\n",
    "            alpha=thisalpha,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=max_iter,\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        y_predict_output = mlpreg.predict(X_predict_input)\n",
    "\n",
    "        thisaxis.set_title(\n",
    "            f\"MLP regression\\nalpha={thisalpha}, activation={thisactivation})\"\n",
    "        )\n",
    "        thisaxis.set_xlabel(\"Input feature\")\n",
    "        thisaxis.set_ylabel(\"Target value\")\n",
    "        thisaxis.set_xlim([-2.5, 0.75])\n",
    "\n",
    "        thisaxis.plot(X_predict_input, y_predict_output, \"^\", markersize=10)\n",
    "        thisaxis.plot(X_train, y_train, \"o\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application to real-world dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=[100, 100], alpha=5.0, random_state=0, solver=\"lbfgs\"\n",
    ").fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Breast cancer dataset\")\n",
    "print(\n",
    "    \"Accuracy of NN classifier on training set: {:.3f}\".format(\n",
    "        clf.score(X_train_scaled, y_train)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of NN classifier on test set: {:.3f}\".format(\n",
    "        clf.score(X_test_scaled, y_test)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
